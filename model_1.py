# -*- coding: utf-8 -*-
"""Model-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A96LK4F5_RZxCpjpQCr0jCQannXXkDji
"""

!pip install torch numpy datasets transformers scikit-learn

pip install -U transformers torch scikit-learn

pip install --upgrade --force-reinstall torch torchvision torchaudio

#Model 1
import pandas as pd
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertForSequenceClassification, get_scheduler
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#Data load
data = pd.read_csv('toxigen.csv')
data = data[data['prompt_label'] == 1].reset_index(drop=True)
X = data['generation']
y = data['group']

#Encoding labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

#Data split
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)

#Tokenization
MODEL_NAME = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)

class ToxiGenDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        tokens = self.tokenizer(
            self.texts.iloc[idx],
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors="pt"
        )
        return {
            'input_ids': tokens['input_ids'].squeeze(0),
            'attention_mask': tokens['attention_mask'].squeeze(0),
            'label': torch.tensor(self.labels[idx], dtype=torch.long)
        }

#Data preparation
BATCH_SIZE = 32
train_dataset = ToxiGenDataset(X_train, y_train, tokenizer)
test_dataset = ToxiGenDataset(X_test, y_test, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)

#Model initilization
model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(label_encoder.classes_)).to(device)
optimizer = AdamW(model.parameters(), lr=3e-5)
lr_scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * 3)
scaler = torch.cuda.amp.GradScaler()

#Train
def train_model(model, train_loader, optimizer, device, epochs=3):
    model.train()
    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")
        for batch in train_loader:
            input_ids = batch['input_ids'].to(device, non_blocking=True)
            attention_mask = batch['attention_mask'].to(device, non_blocking=True)
            labels = batch['label'].to(device, non_blocking=True)

            optimizer.zero_grad()
            with torch.cuda.amp.autocast():
                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            lr_scheduler.step()

train_model(model, train_loader, optimizer, device)

#Evaluation and F1 visual
def evaluate_model(model, test_loader):
    model.eval()
    predictions, true_labels = [], []
    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device, non_blocking=True)
            attention_mask = batch['attention_mask'].to(device, non_blocking=True)
            labels = batch['label'].cpu().numpy()

            with torch.cuda.amp.autocast():
                logits = model(input_ids, attention_mask=attention_mask).logits
            preds = torch.argmax(F.softmax(logits, dim=-1), dim=-1).cpu().numpy()

            predictions.extend(preds)
            true_labels.extend(labels)

    print(f"\nAccuracy: {accuracy_score(true_labels, predictions):.4f}")
    report = classification_report(true_labels, predictions, target_names=label_encoder.classes_, output_dict=True)


    f1_scores = [report[label]['f1-score'] for label in label_encoder.classes_]
    class_labels = label_encoder.classes_


    plt.figure(figsize=(12, 6))
    bars = plt.bar(class_labels, f1_scores, color='skyblue')
    plt.title('F1-score by Class')
    plt.xlabel('Class')
    plt.ylabel('F1-score')
    plt.xticks(rotation=45, ha='right')
    plt.ylim(0, 1.0)

    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.2f}', ha='center', va='bottom', fontsize=8)

    plt.tight_layout()
    plt.show()

evaluate_model(model, test_loader)

#off the thesis - roberta soft voting
import torch
import numpy as np
import pandas as pd
from datasets import Dataset
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)
from collections import Counter
from sklearn.utils import resample

#Data load
df = pd.read_csv("toxigen.csv")
df = df.rename(columns={"group": "label"})
labels = sorted(df.label.unique())
label2id = {label: idx for idx, label in enumerate(labels)}
id2label = {idx: label for label, idx in label2id.items()}
df["label"] = df["label"].map(label2id)

#Train test split
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
train_idx, test_idx = next(splitter.split(df["generation"], df["label"]))
train_df = df.iloc[train_idx].reset_index(drop=True)
test_df = df.iloc[test_idx].reset_index(drop=True)

#Oversampling
min_samples_per_class = 800
balanced_train = []

for label, group_df in train_df.groupby("label"):
    if len(group_df) < min_samples_per_class:
        oversampled = resample(group_df,
                               replace=True,
                               n_samples=min_samples_per_class,
                               random_state=42)
    else:
        oversampled = group_df
    balanced_train.append(oversampled)

train_df = pd.concat(balanced_train).sample(frac=1, random_state=42).reset_index(drop=True)

#Focal Loss
class FocalLoss(torch.nn.Module):
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, logits, labels):
        ce_loss = torch.nn.functional.cross_entropy(logits, labels, reduction="none")
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return torch.mean(focal_loss) if self.reduction == 'mean' else focal_loss

#Trainer
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")


        if logits.ndim != 2:
            logits = logits.view(-1, model.config.num_labels)
        if labels.ndim != 1:
            labels = labels.view(-1)

        loss_fn = FocalLoss(gamma=2)
        loss = loss_fn(logits, labels)
        return (loss, outputs) if return_outputs else loss

#Train args
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=24,
    per_device_eval_batch_size=64,
    num_train_epochs=3,
    logging_dir="./logs",
    save_strategy="no",
    eval_strategy="no",
    logging_steps=500,
    report_to="none",
    fp16=torch.cuda.is_available(),
)

#Tokenization
def tokenize(batch, tokenizer):
    return tokenizer(batch["generation"], truncation=True, padding="max_length", max_length=96)

#Train and predict
def train_and_predict_roberta(seed):
    tokenizer = AutoTokenizer.from_pretrained("roberta-base")

    train_dataset = Dataset.from_pandas(train_df)
    test_dataset = Dataset.from_pandas(test_df)
    train_dataset = train_dataset.map(lambda x: tokenize(x, tokenizer))
    test_dataset = test_dataset.map(lambda x: tokenize(x, tokenizer))
    train_dataset = train_dataset.remove_columns(["generation"])
    test_dataset = test_dataset.remove_columns(["generation"])

    model = AutoModelForSequenceClassification.from_pretrained(
        "roberta-base", num_labels=len(labels)
    )

    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        tokenizer=tokenizer,
    )

    trainer.train()
    preds = trainer.predict(test_dataset).predictions
    return preds

#Train all 3 models
preds_all = []
for seed in [101, 202, 303]:
    preds = train_and_predict_roberta(seed)
    preds_all.append(preds)

#Soft Voting
ensemble_logits = sum(preds_all) / len(preds_all)
ensemble_preds = np.argmax(ensemble_logits, axis=1)
y_true = test_df["label"].values

#Metrics
print("\n Ensemble Classification Report:")
print(classification_report(
    y_true,
    ensemble_preds,
    target_names=[id2label[i] for i in range(len(id2label))],
    digits=4
))

#F1 and visualisation
report = classification_report(
    y_true,
    ensemble_preds,
    target_names=[id2label[i] for i in range(len(id2label))],
    output_dict=True
)
f1_scores = [report[label]["f1-score"] for label in id2label.values()]

plt.figure(figsize=(12, 6))
bars = plt.bar(id2label.values(), f1_scores, color="coral")
plt.ylim(0, 1)
plt.ylabel("F1-score (Ensemble)")
plt.title("F1-score by Class (3x Roberta-base + Oversampling)")
plt.xticks(rotation=45, ha="right")
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f"{yval:.2f}", ha='center', va='bottom', fontsize=9)
plt.tight_layout()
plt.show()

import torch
import numpy as np
import pandas as pd
from datasets import Dataset
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import classification_report
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback
import matplotlib.pyplot as plt
import torch.nn as nn


model_name = "microsoft/deberta-v3-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)


df = pd.read_csv("toxigen.csv")
df = df.rename(columns={"group": "label"})
labels = sorted(df.label.unique())
label2id = {label: idx for idx, label in enumerate(labels)}
id2label = {idx: label for label, idx in label2id.items()}
df["label"] = df["label"].map(label2id)


WEAK_CLASSES = ["latino", "chinese", "mexican"]
WEAK_CLASS_IDS = [label2id[c] for c in WEAK_CLASSES]
df_oversample = pd.concat([df[df["label"] == cid].sample(frac=2.5, replace=True, random_state=42) for cid in WEAK_CLASS_IDS])
df = pd.concat([df, df_oversample]).reset_index(drop=True)


splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
train_idx, test_idx = next(splitter.split(df["generation"], df["label"]))
test_df = df.iloc[test_idx].reset_index(drop=True)
y_true = test_df["label"].values


def tokenize(batch):
    return tokenizer(batch["generation"], truncation=True, padding="max_length", max_length=96)

test_dataset = Dataset.from_pandas(test_df)
test_dataset = test_dataset.map(tokenize)
test_dataset = test_dataset.remove_columns(["generation"])


class_counts = df["label"].value_counts().sort_index()
alpha = 1.0 / class_counts
alpha = (alpha / alpha.sum()).values


class FocalLossTrainer(Trainer):
    def __init__(self, *args, alpha=None, gamma=2.0, **kwargs):
        super().__init__(*args, **kwargs)
        self.alpha = torch.tensor(alpha, dtype=torch.float32).to(self.args.device) if alpha is not None else None
        self.gamma = gamma

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        ce_loss = nn.CrossEntropyLoss(reduction='none')(logits, labels)
        pt = torch.exp(-ce_loss)
        focal_loss = ((1 - pt) ** self.gamma) * ce_loss
        if self.alpha is not None:
            at = self.alpha[labels]
            focal_loss *= at
        return (focal_loss.mean(), outputs) if return_outputs else focal_loss.mean()


training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    num_train_epochs=2.8,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=100,
    report_to="none",
    fp16=torch.cuda.is_available(),
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    gradient_accumulation_steps=1,
    seed=42,
)


def train_model(seed):
    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)
    train_idx, _ = next(splitter.split(df["generation"], df["label"]))
    train_df = df.iloc[train_idx].reset_index(drop=True)
    train_dataset = Dataset.from_pandas(train_df).map(tokenize).remove_columns(["generation"])

    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(labels), hidden_dropout_prob=0.3)
    trainer = FocalLossTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        tokenizer=tokenizer,
        alpha=alpha,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
    )
    trainer.train()
    preds = trainer.predict(test_dataset).predictions
    return preds


preds1 = train_model(seed=101)
preds2 = train_model(seed=202)
preds3 = train_model(seed=303)


ensemble_logits = (preds1 + preds2 + preds3) / 3
ensemble_preds = np.argmax(ensemble_logits, axis=1)


print("\n Ensemble Classification Report:")
print(classification_report(
    y_true,
    ensemble_preds,
    target_names=[id2label[i] for i in range(len(id2label))],
    digits=4
))


report = classification_report(
    y_true,
    ensemble_preds,
    target_names=[id2label[i] for i in range(len(id2label))],
    output_dict=True
)
f1_scores = [report[label]["f1-score"] for label in id2label.values()]

plt.figure(figsize=(12, 6))
bars = plt.bar(id2label.values(), f1_scores, color="coral")
plt.ylim(0, 1)
plt.ylabel("F1-score (Ensemble)")
plt.title("F1-score by Class (Soft Voting Ensemble)")
plt.xticks(rotation=45, ha="right")
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f"{yval:.2f}", ha='center', va='bottom', fontsize=9)
plt.tight_layout()
plt.show()