# -*- coding: utf-8 -*-
"""Model 3 and 6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OQaNbV39t2yUPSEg6gWHI8A19Dwl07rH
"""

!pip install torch numpy datasets transformers scikit-learn

!pip install -U transformers

pip install --upgrade transformers

import transformers
print(transformers.__version__)

!pip install torch numpy datasets transformers scikit-learn

#Model 3
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import StratifiedShuffleSplit

#Data loading
df = pd.read_csv("toxigen.csv")
df = df[df['prompt_label'] == 1].reset_index(drop=True)
df = df[["generation", "group"]].dropna()
df = df[df["group"] != "other"]  # –º–æ–∂–Ω–æ –∏—Å–∫–ª—é—á–∏—Ç—å 'other' –µ—Å–ª–∏ –º–∞–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤

labels = sorted(df["group"].unique())
label2id = {label: i for i, label in enumerate(labels)}
id2label = {i: label for label, i in label2id.items()}
df["label"] = df["group"].map(label2id)

#Stratified Split
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
train_idx, test_idx = next(splitter.split(df["generation"], df["label"]))
train_df = df.iloc[train_idx].reset_index(drop=True)
test_df = df.iloc[test_idx].reset_index(drop=True)

#Model load and tokenization
model_name = "microsoft/deberta-v3-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(example):
    return tokenizer(example["generation"], truncation=True, padding="max_length", max_length=128)

train_dataset = Dataset.from_pandas(train_df).map(tokenize)
test_dataset = Dataset.from_pandas(test_df).map(tokenize)

train_dataset = train_dataset.remove_columns(["generation", "group"])
test_dataset = test_dataset.remove_columns(["generation", "group"])

#Weights
class_weights = compute_class_weight(class_weight="balanced", classes=np.unique(train_df["label"]), y=train_df["label"])
class_weights = torch.tensor(class_weights, dtype=torch.float32)

#Focal Loss
class FocalLoss(torch.nn.Module):
    def __init__(self, alpha=None, gamma=2.0):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(self, inputs, targets):
        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)
        pt = torch.exp(-ce_loss)
        focal_loss = ((1 - pt) ** self.gamma) * ce_loss
        return focal_loss.mean()

model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(labels))

#Metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="macro")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

#Custom trainer
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # ‚Üê –¥–æ–±–∞–≤–∏–ª–∏ **kwargs
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        loss_fct = FocalLoss(alpha=class_weights.to(model.device))
        loss = loss_fct(logits, labels)
        return (loss, outputs) if return_outputs else loss


#Training
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    eval_strategy="epoch",  # –≤–º–µ—Å—Ç–æ evaluation_strategy
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=100,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    fp16=torch.cuda.is_available(),
    report_to=[]  # –æ—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–≥–µ—Ä—ã (wandb –∏ —Ç.–ø.)
)


trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)

trainer.train()

#Evaluation
print("Evaluation metrics:")
metrics = trainer.evaluate()
print(metrics)

#Classification Report
preds = trainer.predict(test_dataset)
y_true = preds.label_ids
y_pred = np.argmax(preds.predictions, axis=1)

print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=[id2label[i] for i in range(len(id2label))], digits=4))

#F1-score
report = classification_report(y_true, y_pred, target_names=[id2label[i] for i in range(len(id2label))], output_dict=True)
f1_scores = [report[label]["f1-score"] for label in report if label in id2label.values()]

plt.figure(figsize=(12, 6))
bars = plt.bar(id2label.values(), f1_scores, color="skyblue")
plt.ylim(0, 1)
plt.ylabel("F1-score")
plt.title("F1-score by Class (ToxiGen Classification)")
plt.xticks(rotation=45, ha="right")

for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f"{yval:.2f}", ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()

!pip install torch numpy datasets transformers scikit-learn

import torch
import numpy as np
import pandas as pd
from datasets import Dataset
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments
from transformers import Trainer

# ‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏
model_name = "microsoft/deberta-v3-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# ‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
df = pd.read_csv("toxigen.csv")
df = df.rename(columns={"group": "label"})
labels = sorted(df.label.unique())
label2id = {label: idx for idx, label in enumerate(labels)}
id2label = {idx: label for label, idx in label2id.items()}
df["label"] = df["label"].map(label2id)

def tokenize(batch):
    return tokenizer(batch["generation"], truncation=True, padding="max_length", max_length=96)  # —Å–æ–∫—Ä–∞—â–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏

# ‚úÖ –¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
train_idx, test_idx = next(splitter.split(df["generation"], df["label"]))
test_df = df.iloc[test_idx].reset_index(drop=True)
test_dataset = Dataset.from_pandas(test_df)
test_dataset = test_dataset.map(tokenize)
test_dataset = test_dataset.remove_columns(["generation"])
y_true = test_df["label"].values

# ‚úÖ –ê—Ä–≥—É–º–µ–Ω—Ç—ã —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=24,  # —É–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    per_device_eval_batch_size=64,
    num_train_epochs=2.5,  # —É–º–µ–Ω—å—à–µ–Ω–æ
    logging_dir="./logs",
    save_strategy="no",
    eval_strategy="no",
    logging_steps=1000,
    report_to="none",
    fp16=torch.cuda.is_available(),
    gradient_accumulation_steps=1  # –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–æ 2, –µ—Å–ª–∏ –ø–∞–º—è—Ç–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç
)

# ‚úÖ –ö–∞—Å—Ç–æ–º–Ω—ã–π Trainer —Å CrossEntropy
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        loss_fct = torch.nn.CrossEntropyLoss()
        loss = loss_fct(logits, labels)
        return (loss, outputs) if return_outputs else loss

# ‚úÖ –§—É–Ω–∫—Ü–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
def train_model(seed):
    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)
    train_idx, _ = next(split.split(df["generation"], df["label"]))
    train_df = df.iloc[train_idx].reset_index(drop=True)

    train_dataset = Dataset.from_pandas(train_df)
    train_dataset = train_dataset.map(tokenize)
    train_dataset = train_dataset.remove_columns(["generation"])

    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(labels))
    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        tokenizer=tokenizer,
    )
    trainer.train()
    preds = trainer.predict(test_dataset).predictions
    return preds

# ‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —Ç—Ä—ë—Ö –º–æ–¥–µ–ª–µ–π
preds1 = train_model(seed=101)
preds2 = train_model(seed=202)
preds3 = train_model(seed=303)

# ‚úÖ –ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ soft voting
ensemble_logits = (preds1 + preds2 + preds3) / 3
ensemble_preds = np.argmax(ensemble_logits, axis=1)

# ‚úÖ –ú–µ—Ç—Ä–∏–∫–∏
print("\nüìä Ensemble Classification Report:")
print(classification_report(
    y_true,
    ensemble_preds,
    target_names=[id2label[i] for i in range(len(id2label))],
    digits=4
))

# ‚úÖ F1-–≥—Ä–∞—Ñ–∏–∫
report = classification_report(
    y_true,
    ensemble_preds,
    target_names=[id2label[i] for i in range(len(id2label))],
    output_dict=True
)
f1_scores = [report[label]["f1-score"] for label in id2label.values()]

plt.figure(figsize=(12, 6))
bars = plt.bar(id2label.values(), f1_scores, color="coral")
plt.ylim(0, 1)
plt.ylabel("F1-score (Ensemble)")
plt.title("F1-score by Class (Soft Voting Ensemble)")
plt.xticks(rotation=45, ha="right")
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f"{yval:.2f}", ha='center', va='bottom', fontsize=9)
plt.tight_layout()
plt.show()

!pip install torch numpy datasets transformers scikit-learn

#Model 6
import torch
import numpy as np
import pandas as pd
from datasets import Dataset
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

#Loading models
model_names = [
    "bert-base-uncased",
    "roberta-base",
    "microsoft/deberta-v3-small"
]

# Data reading
df = pd.read_csv("toxigen.csv")
df = df[df["prompt_label"] == 1].reset_index(drop=True)

df = df.rename(columns={"group": "label"})
labels = sorted(df.label.unique())
label2id = {label: idx for idx, label in enumerate(labels)}
id2label = {idx: label for label, idx in label2id.items()}
df["label"] = df["label"].map(label2id)

# Oversampling
weak_classes = ["women", "asian", "black", "middle_east", "muslim", "latino"]
weak_ids = [label2id[label] for label in weak_classes if label in label2id]
df_oversampled = df.copy()

for label_id in weak_ids:
    class_df = df[df["label"] == label_id]
    df_oversampled = pd.concat([df_oversampled, class_df.sample(n=len(class_df), replace=True)], axis=0)

df_oversampled = df_oversampled.sample(frac=1, random_state=42).reset_index(drop=True)

#Data split
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
train_idx, test_idx = next(splitter.split(df_oversampled["generation"], df_oversampled["label"]))
train_df = df_oversampled.iloc[train_idx].reset_index(drop=True)
test_df = df_oversampled.iloc[test_idx].reset_index(drop=True)

#Data tokenization
def tokenize(batch, tokenizer):
    return tokenizer(batch["generation"], truncation=True, padding="max_length", max_length=96)

y_true = test_df["label"].values

#Custom weights for our weak classes
base_weight = 1.0
boosted_weight = 3.0
custom_weights = []

for label in labels:
    if label in weak_classes:
        custom_weights.append(boosted_weight)
    else:
        custom_weights.append(base_weight)

class_weights = torch.tensor(custom_weights).float()

#Focal Loss
class FocalLoss(torch.nn.Module):
    def __init__(self, alpha=None, gamma=2, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, logits, labels):
        ce_loss = torch.nn.functional.cross_entropy(logits, labels, reduction="none")
        pt = torch.exp(-ce_loss)
        at = self.alpha[labels] if self.alpha is not None else 1.0
        focal_loss = at * (1 - pt) ** self.gamma * ce_loss
        return torch.mean(focal_loss) if self.reduction == 'mean' else focal_loss

#Custom Trainer
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")

        if logits.ndim != 2:
            logits = logits.view(-1, model.config.num_labels)
        if labels.ndim != 1:
            labels = labels.view(-1)

        loss_fn = FocalLoss(alpha=class_weights.to(logits.device), gamma=2)
        loss = loss_fn(logits, labels)
        return (loss, outputs) if return_outputs else loss

#Arguments for training
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=24,
    per_device_eval_batch_size=64,
    num_train_epochs=2.5,
    logging_dir="./logs",
    save_strategy="no",
    eval_strategy="no",
    logging_steps=1000,
    report_to="none",
    fp16=torch.cuda.is_available(),
)

#Trainer func
def train_and_predict(model_name, seed):
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    train_dataset = Dataset.from_pandas(train_df)
    test_dataset = Dataset.from_pandas(test_df)
    train_dataset = train_dataset.map(lambda x: tokenize(x, tokenizer))
    test_dataset = test_dataset.map(lambda x: tokenize(x, tokenizer))
    train_dataset = train_dataset.remove_columns(["generation"])
    test_dataset = test_dataset.remove_columns(["generation"])

    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(labels))

    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        tokenizer=tokenizer,
    )

    trainer.train()
    preds = trainer.predict(test_dataset).predictions
    return preds

preds_all = []
for seed, model_name in zip([101, 202, 303], model_names):
    preds = train_and_predict(model_name, seed)
    preds_all.append(preds)

# Soft Voting
ensemble_logits = sum(preds_all) / len(preds_all)
ensemble_preds = np.argmax(ensemble_logits, axis=1)

#Metrics
print("\n Ensemble Classification Report:")
print(classification_report(
    y_true,
    ensemble_preds,
    target_names=[id2label[i] for i in range(len(id2label))],
    digits=4
))

#F1-class.report
report = classification_report(
    y_true,
    ensemble_preds,
    target_names=[id2label[i] for i in range(len(id2label))],
    output_dict=True
)
f1_scores = [report[label]["f1-score"] for label in id2label.values()]

plt.figure(figsize=(12, 6))
bars = plt.bar(id2label.values(), f1_scores, color="coral")
plt.ylim(0, 1)
plt.ylabel("F1-score (Ensemble)")
plt.title("F1-score by Class (Soft Voting Ensemble - 3 models)")
plt.xticks(rotation=45, ha="right")
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f"{yval:.2f}", ha='center', va='bottom', fontsize=9)
plt.tight_layout()
plt.show()